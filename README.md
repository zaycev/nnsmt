Word Bases MT System with NN
============================

#### Generative Story

```
1. i <- 1, j <- 0
2. Choose \delta j from [-h, h] with the following probability:
	 P_d(\delta j | f_{j - h}, ..., f_{j+h}, e_{i-g}, ..., e_{i + j})
3. Let j <- j + \delta j
4. Generate e_i with probability:
    P_t(e_i | f_{j - h}, ..., f_{j+h}, e_{i-g}, ..., e_{i + j})
5. If e_i = </s>, stop; otherwise go to 2.
```

### 0. Data Preparation

Input data is Target-Source parallel corpus, word-aligned using GIZA++. You can test this MT pipeline with a sample dataset stored in contrib `directory`.

Before run the training pipeline, first open `sbin/run-env.sh` and change environment variables `WORK_DIR` to make it point to your working directory path.


### 1. Word Alignment

Word alignments are given (generated by GIZA++ or any other tool), but our model requires that every target word has to be aligned with exactly one not null source word. The algorithm of PP rules is explained in [4,3], here is a brief idea:

**Affiliation Heuristic**
* If `t_i` aligns to exactly one source word, `A_i` is the index of the word it aligns to.
* If `t_i` align to multiple source words, `A_i` is the index of the aligned word in the middle (round
down).
* If `t_i` is unaligned, we inherit its affiliation from the closest aligned word, starting with the right.


**PP Rules**

If a source word is aligned with multiple target words which are not consecutive, first the link to the least frequent target word is identified, and the group of links containing this word is retained while the others are deleted.

The intuition here is to keep the alignments containing content words (which are less frequent than functional words).

The new alignment has no targetside discontinuities anymore, but might still contain unaligned target words.

For each unaligned target word, we determine the (left or right) neighbour that it appears more frequently with and align it with the same source word as the neighbour.

The result is an alignment without target-side discontinuities and unaligned target words.

![PP Rule Example](https://raw.github.com/zaycev/nnsmt/master/media/durrani_pp_aligment.png "PP Rule Example")

### 2. Training MT Model

Generate training examples for the distortion and translation model.
Train the neural networks using NPLM toolkit.

### 3. Decoding

The design of the decoder would be very similar to a phrase-based decoder, except that we only generate a single English word at a time.

### Related Links

1. N. Durrani, A. Fraser, H. Schmid. 2013. [Model With Minimal Translation Units, But DecodeWith Phrases](http://www.cis.uni-muenchen.de/~fraser/pubs/durrani_naacl2013.pdf)
2. H. Zhang, K. Toutanova, C. Quirk, J. Gao. 2013. [Beyond Left-to-Right: Multiple Decomposition Structures for SMT](http://research.microsoft.com/en-us/um/people/jfgao/paper/2013/mtu.pdf)
3. N. Durrani, H. Schmid, A. Fraser. 2011. [A Joint Sequence Translation Model with Integrated Reordering](http://aclweb.org/anthology//P/P11/P11-1105.pdf)
4. J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, J. Makhoul. 2014. Fast and Robust Neural Network Joint Models for Statistical Machine Translation.

